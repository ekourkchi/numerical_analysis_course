<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>11 Random Numbers
and Stochastic Methods</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<!-- html,2 --> 
<meta name="src" content="compphysics.tex"> 
<meta name="date" content="2015-12-09 14:48:00"> 
<link rel="stylesheet" type="text/css" href="compphysics.css"> 
</head><body 
>
   <!--l. 1--><div class="crosslinks"><p class="noindent">[<a 
href="compphysicsch12.html" >next</a>] [<a 
href="compphysicsch10.html" >prev</a>] [<a 
href="compphysicsch10.html#tailcompphysicsch10.html" >prev-tail</a>] [<a 
href="#tailcompphysicsch11.html">tail</a>] [<a 
href="compphysics.html#compphysicsch11.html" >up</a>] </p></div>
   <h2 class="chapterHead"><span class="titlemark">&#x00A0;11</span><br /><a 
 id="x13-4300011"></a>Random Numbers<br 
class="newline" />and Stochastic Methods</h2>
   <h3 class="sectionHead"><span class="titlemark">11.1   </span> <a 
 id="x13-4400011.1"></a>Generation of probabilistic distributions</h3>
<!--l. 5--><p class="noindent" >Random number generators are not truly random, but use deterministic rules to
generate &#8220;pseudorandom&#8221; numbers, for example <span 
class="cmmi-12">x</span><sub><span 
class="cmmi-8">i</span><span 
class="cmr-8">+1</span></sub> = (23<span 
class="cmmi-12">x</span><sub><span 
class="cmmi-8">i</span></sub>)mod(10<sup><span 
class="cmr-8">8</span></sup> + 1),
meaning the remainder of 23<span 
class="cmmi-12">x</span><sub><span 
class="cmmi-8">i</span></sub><span 
class="cmmi-12">&#x2215;</span>100000001. The starting value <span 
class="cmmi-12">x</span><sub><span 
class="cmr-8">0</span></sub> is called the
&#8220;seed.&#8221; Pseudorandom number generators can never ideally satisfy all desired
statistical properties. For example, since there are only finitely many computer
representable numbers they will ultimately always be periodic, though the
period can be extremely long. Random number generators are said to be
responsible for many wrong computational results. Particular choices of
the seed can lead to short periods. Likewise, the coefficients in formulas
like the one above need to be chosen carefully. Many implementations of
pseudorandom number generators were simply badly chosen or faulty. The
situation has however improved and current random number generators
suffice for almost any practical purpose. Source code routines seem to be
universally better than built-in random number generators provided by
libraries.
<!--l. 7--><p class="indent" >   Pseudorandom number generators produce a uniform distribution of
numbers in an interval, typically either integers or real numbers in the
interval from 0 to 1 (without perhaps one or both of the endpoints). How do
we obtain a different distribution? A new probability distribution, <span 
class="cmmi-12">p</span>(<span 
class="cmmi-12">x</span>),
can be related to a given one, <span 
class="cmmi-12">q</span>(<span 
class="cmmi-12">y</span>), by a transformation <span 
class="cmmi-12">y </span>= <span 
class="cmmi-12">y</span>(<span 
class="cmmi-12">x</span>). The
probability to be between <span 
class="cmmi-12">x </span>and <span 
class="cmmi-12">x </span>+ <span 
class="cmmi-12">dx </span>is <span 
class="cmmi-12">p</span>(<span 
class="cmmi-12">x</span>)<span 
class="cmmi-12">dx</span>. By construction, this equals
the probability to be between <span 
class="cmmi-12">y </span>and <span 
class="cmmi-12">y </span>+ <span 
class="cmmi-12">dy</span>. Hence, <span 
class="cmsy-10x-x-120">|</span><span 
class="cmmi-12">p</span>(<span 
class="cmmi-12">x</span>)<span 
class="cmmi-12">dx</span><span 
class="cmsy-10x-x-120">| </span>= <span 
class="cmsy-10x-x-120">|</span><span 
class="cmmi-12">q</span>(<span 
class="cmmi-12">y</span>)<span 
class="cmmi-12">dy</span><span 
class="cmsy-10x-x-120">|</span>,
where the absolute values are needed because <span 
class="cmmi-12">y </span>could decrease with <span 
class="cmmi-12">x</span>,
while probabilities are always positive. If <span 
class="cmmi-12">q</span>(<span 
class="cmmi-12">y</span>) is uniformly distributed
between 0 and 1, then <span 
class="cmmi-12">p</span>(<span 
class="cmmi-12">x</span>) = <span 
class="cmsy-10x-x-120">|</span><span 
class="cmmi-12">dy&#x2215;dx</span><span 
class="cmsy-10x-x-120">| </span>for 0 <span 
class="cmmi-12">&#x003C; y &#x003C; </span>1 and otherwise <span 
class="cmmi-12">p</span>(<span 
class="cmmi-12">x</span>) = 0.
Integration with respect to <span 
class="cmmi-12">x </span>and inverting yields the desired transformation.
For example, an exponential distribution <span 
class="cmmi-12">p</span>(<span 
class="cmmi-12">x </span><span 
class="cmsy-10x-x-120">&#x2265; </span>0) = exp(<span 
class="cmsy-10x-x-120">-</span><span 
class="cmmi-12">x</span>) requires
<span 
class="cmmi-12">y</span>(<span 
class="cmmi-12">x</span>) = <span 
class="cmex-10x-x-120">&#x222B;</span>
  <span 
class="cmmi-12">p</span>(<span 
class="cmmi-12">x</span>)<span 
class="cmmi-12">dx </span>= <span 
class="cmsy-10x-x-120">-</span> exp(<span 
class="cmsy-10x-x-120">-</span><span 
class="cmmi-12">x</span>) + <span 
class="cmmi-12">C </span>and therefore <span 
class="cmmi-12">x</span>(<span 
class="cmmi-12">y</span>) = <span 
class="cmsy-10x-x-120">-</span> ln(<span 
class="cmmi-12">C </span><span 
class="cmsy-10x-x-120">-</span><span 
class="cmmi-12">y</span>). With <span 
class="cmmi-12">C </span>= 1
the distribution has the proper bounds. This transforms uniformly distributed
numbers to exponentially distributed numbers. In general, it is necessary to
                                                                          

                                                                          
invert the integral of the desired distribution function <span 
class="cmmi-12">p</span>(<span 
class="cmmi-12">x</span>). That can be
computationally expensive, particularly when the inverse cannot be obtained
analytically.
<!--l. 15--><p class="indent" >   Alternatively the desired distribution <span 
class="cmmi-12">p</span>(<span 
class="cmmi-12">x</span>) can be enforced by rejecting
numbers with a probability 1 <span 
class="cmsy-10x-x-120">-</span><span 
class="cmmi-12">p</span>(<span 
class="cmmi-12">x</span>), using a second randomly generated number.
These two methods are called &#8220;transformation method&#8221; and &#8220;rejection method,&#8221;
respectively.
<!--l. 18--><p class="noindent" ><span 
class="cmbx-12">Recommended Reading: </span>For generation and testing of random numbers
see Knuth, <span 
class="cmti-12">The Art of Computer Programming</span>, Vol.&#x00A0;2. Methods for
generating probability distributions are found in Devroye, <span 
class="cmti-12">Non-Uniform</span>
<span 
class="cmti-12">Random Variate Generation</span>, which is also available on the web at
<a 
href="http://luc.devroye.org/rnbookindex.html" class="url" ><span 
class="cmtt-12">http://luc.devroye.org/rnbookindex.html</span></a>.
<!--l. 26--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">11.2   </span> <a 
 id="x13-4500011.2"></a>Monte Carlo integration: accuracy through randomness</h3>
<!--l. 28--><p class="noindent" >Besides obvious uses of random numbers there are numerical methods that
intrinsically rely on probabilistic means. A representative example is the Monte
Carlo algorithm for multi-dimensional integration.
<!--l. 30--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x13-450011"></a>
                                                                          

                                                                          
<!--l. 31--><p class="noindent" ><img 
src="compphysics55x.png" alt="PIC" class="graphics" width="170.71652pt" height="149.3852pt" ><!--tex4ht:graphics  
name="compphysics55x.png" src="monte.eps"  
--> <br />  <div class="caption" 
><span class="id">Figure&#x00A0;11.1:  </span><span  
class="content">Randomly  distributed
 points are used to estimate the area
 below the graph. </span></div><!--tex4ht:label?: x13-450011 -->
                                                                          

                                                                          
<!--l. 35--><p class="indent" >   </div><hr class="endfigure">
<!--l. 37--><p class="indent" >   Consider the following method for one-dimensional integration. We choose
random coordinates <span 
class="cmmi-12">x </span>and <span 
class="cmmi-12">y</span>, evaluate the function at <span 
class="cmmi-12">x</span>, and see whether <span 
class="cmmi-12">y </span>is
below or above the graph of the function; see figure&#x00A0;<a 
href="#x13-450011">11.1<!--tex4ht:ref: fig:monte-carlo --></a>. If this is repeated with
many more points over a region, then the fraction of points that fall below the
graph is an estimate for the area under the graph relative to the area of the entire
region. This is Monte Carlo integration.
<!--l. 39--><p class="indent" >   Intuitively this seems to be a crude method for integration, because it discards
the actual value of the function. Let&#8217;s quantify the error of this numerical
scheme.
<!--l. 41--><p class="indent" >   Suppose we choose <span 
class="cmmi-12">N </span>randomly distributed points, requiring <span 
class="cmmi-12">N </span>function
evaluations. How fast does the integration error decrease with <span 
class="cmmi-12">N</span>? The probability
of a random point to be below the graph is proportional to the area <span 
class="cmmi-12">a </span>under the
graph. Without loss of the generality, the constant of proportionality can
be set to one. The probability <span 
class="cmmi-12">P </span>of having <span 
class="cmmi-12">m </span>points below the graph
and <span 
class="cmmi-12">N </span><span 
class="cmsy-10x-x-120">- </span><span 
class="cmmi-12">m </span>points above the graph is given by a binomial distribution,
<span 
class="cmmi-12">P</span>(<span 
class="cmmi-12">m</span>) = <span 
class="cmex-10x-x-120">(</span><span 
class="cmmi-8">n</span>
   <span 
class="cmmi-8">m</span><span 
class="cmex-10x-x-120">)</span>
   <span 
class="cmmi-12">a</span><sup><span 
class="cmmi-8">m</span></sup>(1 <span 
class="cmsy-10x-x-120">- </span><span 
class="cmmi-12">a</span>)<sup><span 
class="cmmi-8">N</span><span 
class="cmsy-8">-</span><span 
class="cmmi-8">m</span></sup>. An error <span 
class="cmmi-12">E </span>can be defined as the root mean
square difference between the exact area <span 
class="cmmi-12">a </span>and the estimated area <span 
class="cmmi-12">m&#x2215;N</span>:
<span 
class="cmmi-12">E</span><sup><span 
class="cmr-8">2</span></sup> = <span 
class="cmex-10x-x-120">&#x2211;</span><sub>
<span 
class="cmmi-8">m</span><span 
class="cmr-8">=0</span></sub><sup><span 
class="cmmi-8">N</span></sup>(<span 
class="cmmi-12">m&#x2215;N </span><span 
class="cmsy-10x-x-120">-</span><span 
class="cmmi-12">a</span>)<sup><span 
class="cmr-8">2</span></sup><span 
class="cmmi-12">P</span>(<span 
class="cmmi-12">m</span>). This sum is <span 
class="cmmi-12">E</span><sup><span 
class="cmr-8">2</span></sup> = (1 <span 
class="cmsy-10x-x-120">-</span><span 
class="cmmi-12">a</span>)<span 
class="cmmi-12">a&#x2215;N</span>. When the integral
is estimated from <span 
class="cmmi-12">N </span>sample points, the error <span 
class="cmmi-12">E </span>is proportional to 1<span 
class="cmmi-12">&#x2215;</span><img 
src="compphysics56x.png" alt="&#x221A; ---
  N"  class="sqrt" >. For
integration in two or more rather than one variable, the exact same calculation
applies.
<!--l. 43--><p class="indent" >   A conventional summation technique to evaluate the integral has an
error too, due to discretization. With a step size of <span 
class="cmmi-12">h</span>, the error would
be typically <span 
class="cmmi-12">O</span>(<span 
class="cmmi-12">h</span><sup><span 
class="cmr-8">2</span></sup>) or <span 
class="cmmi-12">O</span>(<span 
class="cmmi-12">h</span><sup><span 
class="cmr-8">4</span></sup>), depending on the integration scheme. In
one dimension, <span 
class="cmmi-12">h </span>is proportional to 1<span 
class="cmmi-12">&#x2215;N </span>and it makes no sense to use
Monte Carlo integration instead of conventional numerical integration
techniques.
<!--l. 46--><p class="indent" >   In more than one variable, conventional integration requires more function
evaluations to achieve a sufficient resolution in all directions. For <span 
class="cmmi-12">N </span>function
evaluations and <span 
class="cmmi-12">d </span>variables the grid spacing <span 
class="cmmi-12">h </span>is proportional to <span 
class="cmmi-12">N</span><sup><span 
class="cmsy-8">-</span><span 
class="cmr-8">1</span><span 
class="cmmi-8">&#x2215;d</span></sup>.
Hence, in many dimensions, the error decreases extremely slowly with the
number of function evaluations. With, say, a million function evaluations
for an integral over six variables, there will only be 10 grid points along
each axis. The accuracy of Monte Carlo integration, on the other hand, is
the same for any number of integration variables. It is more efficient to
distribute the one million points randomly and measure the integral in this
way.
                                                                          

                                                                          
<!--l. 49--><p class="indent" >   Once can speak of a &#8220;dimension barrier,&#8221; because the number of function
evaluations required for a certain accuracy increases exponentially with the
number of dimensions. Another way to put it: It&#8217;s not that Monte Carlo
Integration is fast, it&#8217;s that multi-dimensional integrals are so slow.
<!--l. 53--><p class="indent" >   Statistical methods can be used efficiently to solve deterministic problems!
<!--l. 55--><p class="indent" >   Another advantage of Monte Carlo integration in many dimensions is that
irregularly shaped boundaries can be easily accommodated.
   <h3 class="sectionHead"><span class="titlemark">11.3   </span> <a 
 id="x13-4600011.3"></a>Ising model<sup><span 
class="cmsy-8">*</span></sup></h3>
<!--l. 63--><p class="noindent" >The Ising model consists of a regular lattice where each site has a &#8220;spin&#8221; which
points up or down. The spins are thought of as magnets that interact with
each other. The energy <span 
class="cmmi-12">E </span>at each site is in this model determined by the
nearest neighbors (n.n.)&#x00A0;only: <span 
class="cmmi-12">E</span><sub><span 
class="cmmi-8">i</span></sub> = <span 
class="cmsy-10x-x-120">-</span><span 
class="cmmi-12">J</span> <span 
class="cmex-10x-x-120">&#x2211;</span>
  <sub><span 
class="cmr-8">(</span><span 
class="cmmi-8">n.n.</span><span 
class="cmr-8">)</span></sub><span 
class="cmmi-12">s</span><sub><span 
class="cmmi-8">i</span></sub><span 
class="cmmi-12">s</span><sub><span 
class="cmmi-8">j</span></sub>, where the spin <span 
class="cmmi-12">s </span>is +1
or <span 
class="cmsy-10x-x-120">-</span>1, and <span 
class="cmmi-12">J </span>is a positive constant. The lattice can be in one, two, or
more dimensions. In one dimension there are two nearest neighbors, on a
two-dimensional square lattice, four nearest neighbors, and so on. There is no real
physical system that behaves exactly this way, but it is a simple model for
the thermodynamics of an interacting system. Ferromagnetism is the
closest physical analog. (Like magnetic poles repel each other, and the
energy is lowest when neighboring magnets have opposite orientations.
Hence, it would appear we should choose <span 
class="cmmi-12">J &#x003C; </span>0 in our model. However,
electrons in metals interact in several ways and in ferromagnetic materials the
energies sum up to align electron dipoles. For this reason we consider
<span 
class="cmmi-12">J &#x003E; </span>0.)
<!--l. 66--><p class="indent" >   The spins have the tendency to align with each other to minimize energy, but
this is counteracted by thermal fluctuations. At zero temperature all spins will
align in the same orientation to reach minimum energy (either all up or all down,
depending on the initial state). At nonzero temperatures will there be relatively
few spins opposite to the overall orientation of spins, or will there be a
roughly equal number of up and down spins? In the former case there is
macroscopic magnetization; in the latter case the average magnetization
vanishes.
<!--l. 68--><p class="indent" >   According to the laws of statistical mechanics the probability to occupy a state
with energy <span 
class="cmmi-12">E </span>is proportional to exp(<span 
class="cmsy-10x-x-120">-</span><span 
class="cmmi-12">E&#x2215;kT</span>), where <span 
class="cmmi-12">k </span>is the Boltzmann
constant and <span 
class="cmmi-12">T </span>the temperature. In equilibrium the number of transitions
from up to down equals the number of transitions from down to up. Let
                                                                          

                                                                          
<span 
class="cmmi-12">W</span>(+ <span 
class="cmsy-10x-x-120">&#x2192;-</span>) denote the probability for a flip from spin up to spin down.
Since in steady state the probability <span 
class="cmmi-12">P </span>of an individual spin to be in one
state is proportional to the number of sites in that state, the equilibrium
condition translates into <span 
class="cmmi-12">P</span>(+)<span 
class="cmmi-12">W</span>(+ <span 
class="cmsy-10x-x-120">&#x2192;-</span>) = <span 
class="cmmi-12">P</span>(<span 
class="cmsy-10x-x-120">-</span>)<span 
class="cmmi-12">W</span>(<span 
class="cmsy-10x-x-120">-&#x2192; </span>+). For a
simulation to reproduce the correct thermodynamic behavior, we hence need
<span 
class="cmmi-12">W</span>(+ <span 
class="cmsy-10x-x-120">&#x2192;-</span>)<span 
class="cmmi-12">&#x2215;W</span>(<span 
class="cmsy-10x-x-120">-&#x2192; </span>+) = <span 
class="cmmi-12">P</span>(<span 
class="cmsy-10x-x-120">-</span>)<span 
class="cmmi-12">&#x2215;P</span>(+) = exp[<span 
class="cmsy-10x-x-120">-</span>(<span 
class="cmmi-12">E</span>(<span 
class="cmsy-10x-x-120">-</span>) <span 
class="cmsy-10x-x-120">-</span><span 
class="cmmi-12">E</span>(+))<span 
class="cmmi-12">&#x2215;kT</span>]. For the Ising
model this ratio is exp(2<span 
class="cmmi-12">bJ&#x2215;kT</span>), where <span 
class="cmmi-12">b </span>is an integer that depends on the
orientations of the nearest neighbors. There is more than one possibility
to choose the transition probabilities <span 
class="cmmi-12">W</span>(+ <span 
class="cmsy-10x-x-120">&#x2192;-</span>) and <span 
class="cmmi-12">W</span>(<span 
class="cmsy-10x-x-120">-&#x2192; </span>+) to
achieve the required ratio. Any of them will lead to the same equilibrium
properties. Call the energy difference between before and after a spin
flip &#x0394;<span 
class="cmmi-12">E</span>, defined to be positive when the energy increases. One possible
choice is to flip from the lower-energy state to the higher-energy state
with probability exp(<span 
class="cmsy-10x-x-120">-</span>&#x0394;<span 
class="cmmi-12">E&#x2215;kT</span>) and to flip from the higher-energy state
to the lower-energy state with probability one. If &#x0394;<span 
class="cmmi-12">E </span>= 0, when there
are equally many neighbors pointing in the up and down direction, then
the transition probability is taken to be one, because this is the limit
for both of the preceding two rules. Ideas have names and this method,
which transitions to a higher energy with probability exp(<span 
class="cmsy-10x-x-120">-</span>&#x0394;<span 
class="cmmi-12">E&#x2215;kT</span>) and
falls to a lower energy with probability 1, is known as the &#8220;Metropolis
algorithm.&#8221;
<!--l. 73--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x13-460012"></a>
                                                                          

                                                                          
<!--l. 74--><p class="noindent" >(a)<img 
src="compphysics57x.png" alt="PIC" class="graphics" width="167.87125pt" height="93.26224pt" ><!--tex4ht:graphics  
name="compphysics57x.png" src="ising1.eps"  
-->
(b)<img 
src="compphysics58x.png" alt="PIC" class="graphics" width="167.87125pt" height="93.26224pt" ><!--tex4ht:graphics  
name="compphysics58x.png" src="ising2.eps"  
-->
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;11.2:  </span><span  
class="content">Magnetization  versus  temperature  from  simulations  of  the
Ising  model  in  (a)  one  dimension  (squares)  and  (b)  two  dimensions
(diamonds). The dashed line shows the analytic solution for an infinitely
large two-dimensional system, [1 <span 
class="cmsy-10x-x-120">- </span>1<span 
class="cmmi-12">&#x2215;</span> sinh <sup><span 
class="cmr-8">4</span></sup>(2<span 
class="cmmi-12">J&#x2215;kT</span>)]<sup><span 
class="cmr-8">1</span><span 
class="cmmi-8">&#x2215;</span><span 
class="cmr-8">8</span></sup>. </span></div><!--tex4ht:label?: x13-460012 -->
                                                                          

                                                                          
<!--l. 78--><p class="indent" >   </div><hr class="endfigure">
<!--l. 81--><p class="indent" >   Such a simulation requires only a short program, which uses a random
number generator. As intended the program produces the following flips for
the one-dimensional model (considering the middle one of three spins):
+ <span 
class="cmsy-10x-x-120">- </span>+ <span 
class="cmsy-10x-x-120">&#x2192; </span>+ + +, + + <span 
class="cmsy-10x-x-120">-&#x2192; </span>+ <span 
class="cmsy-10x-x-120">--</span>, and either + + + <span 
class="cmsy-10x-x-120">&#x2192; </span>+ + + or + + + <span 
class="cmsy-10x-x-120">&#x2192; </span>+ <span 
class="cmsy-10x-x-120">- </span>+.
The last transition never occurs at zero temperature. For <span 
class="cmmi-12">T </span>= 0 the exponent
diverges, but the computer may even handle that correctly.
<!--l. 90--><p class="indent" >   Figure&#x00A0;<a 
href="#x13-460012">11.2<!--tex4ht:ref: fig:isingMvsT --></a> shows the magnetization as a function of temperature obtained
with such a program. Part (a) is for the one-dimensional Ising model and the
spins are initialized in random orientations. The scatter of points at low
temperatures arises from insufficient equilibration and averaging times. In one
dimension the magnetization vanishes for any temperature larger than
zero.
<!--l. 92--><p class="indent" >   But in two dimensions there are two phases. Part (b) of figure&#x00A0;<a 
href="#x13-460012">11.2<!--tex4ht:ref: fig:isingMvsT --></a> shows the
magnetization for the two-dimensional Ising model, where initially all spins point
up. At low temperatures there is magnetization, but at high temperatures the
magnetization vanishes. The two phases are separated by a continuous, not a
discontinuous, change in magnetization. As the system size increases this
transition becomes more and more sharply defined. For an infinite system, the
magnetization vanishes beyond a specific temperature <span 
class="cmmi-12">T</span><sub><span 
class="cmmi-8">c</span></sub> <span 
class="cmsy-10x-x-120">&#x2248; </span>2<span 
class="cmmi-12">.</span>269<span 
class="cmmi-12">J&#x2215;k</span>, the &#8220;critical
temperature.&#8221;
<!--l. 94--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                          

                                                                          
<a 
 id="x13-460023"></a>
                                                                          

                                                                          
<div class="center" 
>
<!--l. 95--><p class="noindent" >
<!--l. 96--><p class="noindent" >(a)<img 
src="compphysics59x.png" alt="PIC" class="graphics" width="105.27519pt" height="104.97696pt" ><!--tex4ht:graphics  
name="compphysics59x.png" src="isiT=2b.eps"  
--> (b)<img 
src="compphysics60x.png" alt="PIC" class="graphics" width="105.27519pt" height="104.97696pt" ><!--tex4ht:graphics  
name="compphysics60x.png" src="isiT=2.3b.eps"  
-->
(c)<img 
src="compphysics61x.png" alt="PIC" class="graphics" width="105.27519pt" height="104.97696pt" ><!--tex4ht:graphics  
name="compphysics61x.png" src="isiT=5b.eps"  
--></div>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;11.3: </span><span  
class="content">Snapshot of spin configurations for the Ising model (a) below,
(b) close to, and (c) above the critical temperature. Black indicates positive
spins, white negative spins. </span></div><!--tex4ht:label?: x13-460023 -->
                                                                          

                                                                          
<!--l. 102--><p class="indent" >   </div><hr class="endfigure">
<!--l. 104--><p class="indent" >   Figure&#x00A0;<a 
href="#x13-460023">11.3<!--tex4ht:ref: fig:isingspins --></a> shows snapshots of the spin configuration in the two-dimensional
Ising model, at temperatures below, close to, and above the phase transition. At
low temperatures, panel&#x00A0;(a), most spins are aligned in the same direction, with a
few exceptions. Occasionally there are individual spins that flip. In this regime,
spins correlate over long distances, although the interactions include only
nearest neighbors. At higher temperatures there are more fluctuations
and more spins of the opposite orientation. Above the phase transition,
panel&#x00A0;(c), there are a roughly equal number of spins up and down, clustered in
small groups. The fluctuations dominate and there is no macroscopic
magnetization.
<!--l. 107--><p class="indent" >   For the Ising model the total energy of the system can change with time. Call
&#x03A9;(<span 
class="cmmi-12">E</span>) the number of spin configurations with total energy <span 
class="cmmi-12">E</span>, where <span 
class="cmmi-12">E </span>is the sum of
the energies of all individual spins, <span 
class="cmmi-12">E </span>= <span 
class="cmex-10x-x-120">&#x2211;</span>
  <sub><span 
class="cmmi-8">j</span></sub><span 
class="cmmi-12">E</span><sub><span 
class="cmmi-8">j</span></sub>. The probability to find the system
in energy <span 
class="cmmi-12">E </span>is proportional to &#x00A0;&#x03A9;(<span 
class="cmmi-12">E</span>) <span 
class="cmex-10x-x-120">&#x220F;</span>
  <sub><span 
class="cmmi-8">j</span></sub> exp(<span 
class="cmsy-10x-x-120">-</span><span 
class="cmmi-12">E</span><sub><span 
class="cmmi-8">j</span></sub><span 
class="cmmi-12">&#x2215;kT</span>) = &#x03A9;(<span 
class="cmmi-12">E</span>) exp(<span 
class="cmsy-10x-x-120">-</span><span 
class="cmmi-12">E&#x2215;kT</span>).
This expression can also be written as exp(<span 
class="cmsy-10x-x-120">-</span><span 
class="cmmi-12">F&#x2215;kT</span>) with <span 
class="cmmi-12">F </span>= <span 
class="cmmi-12">E </span><span 
class="cmsy-10x-x-120">-</span><span 
class="cmmi-12">kT</span> ln &#x03A9;(<span 
class="cmmi-12">E</span>). For
a given temperature, the system is thus most often in a configuration that
minimizes <span 
class="cmmi-12">F</span>, because it makes the exponential largest. The quantity <span 
class="cmmi-12">F </span>becomes
smaller when the energy is lowered or when <span 
class="cmmi-12">k</span> ln &#x03A9;(<span 
class="cmmi-12">E</span>), also called entropy,
increases. Low energy configurations prevail at low temperature and form the
ordered phase of the system; high entropy configurations prevail at high
temperature and form the disordered phase.
<!--l. 110--><p class="indent" >   The thermodynamic properties of the Ising model can be obtained analytically
if one manages to explicitly count the number of possible configurations as a
function energy. This can be done in one dimension, but in two dimensions it is
much, much, much harder. The dashed line in figure&#x00A0;<a 
href="#x13-460023">11.3<!--tex4ht:ref: fig:isingspins --></a>(b) shows this exact
solution, first obtained by Lars Onsager. In three dimensions no one has achieved
it, and hence we believe that it is impossible to do so. The underlying
reason why the magnetization can be obtained with many fewer steps with
the Metropolis algorithm than by counting states, is that a probabilistic
method automatically explores the most likely configurations. The historical
significance of the Ising model stems largely from its <span 
class="cmti-12">analytic </span>solution.
Hence, our numerical attempts in one and two dimensions have had a
merely illustrative nature. But even simple variations of the model (e.g.,
the Ising model in three dimensions or extending the interaction beyond
nearest neighbors) are not solved analytically. In these cases numerics is
valuable and no more difficult than the simulations we have gone through
here.
                                                                          

                                                                          
                                                                          

                                                                          
                                                                          

                                                                          
                                                                          

                                                                          
   <!--l. 1--><div class="crosslinks"><p class="noindent">[<a 
href="compphysicsch12.html" >next</a>] [<a 
href="compphysicsch10.html" >prev</a>] [<a 
href="compphysicsch10.html#tailcompphysicsch10.html" >prev-tail</a>] [<a 
href="compphysicsch11.html" >front</a>] [<a 
href="compphysics.html#compphysicsch11.html" >up</a>] </p></div>
<!--l. 1--><p class="indent" >   <a 
 id="tailcompphysicsch11.html"></a>   
</body></html> 
